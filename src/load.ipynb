{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e5b00d-4c94-45c7-8e77-c4f3cb21372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits (rows): 177 60 60\n",
      "Pos rate (train/val/test): 0.458 0.467 0.467\n",
      "Transformed train shape: (177, 25)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "# Reproducible splits: 60/20/20 (train/val/test)\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "VAL_SIZE = 0.25  # of the 80% temp set -> overall 20%\n",
    "\n",
    "# Columns for the Heart dataset (Cleveland-style)\n",
    "HEART_CATEGORICAL: List[str] = [\"cp\", \"restecg\", \"slope\", \"ca\", \"thal\"]\n",
    "HEART_BINARIES: List[str] = [\"sex\", \"fbs\", \"exang\"]\n",
    "HEART_NUMERIC: List[str] = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "def _split_stratified(X: pd.DataFrame, y: pd.Series, seed: int = RANDOM_STATE):\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=seed\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=VAL_SIZE, stratify=y_temp, random_state=seed\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def _build_preprocessor_heart(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"One-hot encode categoricals; scale numeric + binary.\"\"\"\n",
    "    cat_cols = [c for c in HEART_CATEGORICAL if c in X.columns]\n",
    "    num_cols = [c for c in HEART_NUMERIC + HEART_BINARIES if c in X.columns]\n",
    "\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "            (\"num\", StandardScaler(), num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return preproc\n",
    "\n",
    "def load_heart(csv_path: Path | None = None) -> Tuple[pd.DataFrame, ...]:\n",
    "    \"\"\"\n",
    "    Load and prep the Heart dataset where the target column is named 'condition'.\n",
    "    Returns: X_train, X_val, X_test, y_train, y_val, y_test, preprocessor\n",
    "    \"\"\"\n",
    "    if csv_path is None:\n",
    "        csv_path = DATA_DIR / \"heart.csv\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Standardize target name to 'target'\n",
    "    if \"condition\" in df.columns:\n",
    "        df = df.rename(columns={\"condition\": \"target\"})\n",
    "    elif \"target\" not in df.columns and \"num\" in df.columns:\n",
    "        # Safety: if someone provides a 'num' version (0–4), binarize >0\n",
    "        df[\"target\"] = (pd.to_numeric(df[\"num\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "        df = df.drop(columns=[\"num\"])\n",
    "    elif \"target\" not in df.columns:\n",
    "        raise ValueError(\"Expected a 'condition', 'target', or 'num' column for the label.\")\n",
    "\n",
    "    # Clean any legacy missing markers like '?'\n",
    "    df = df.replace(\"?\", np.nan)\n",
    "\n",
    "    # Split features/label\n",
    "    y = df[\"target\"].astype(int)\n",
    "    X = df.drop(columns=[\"target\"])\n",
    "\n",
    "    # Cast types for preprocessing\n",
    "    for col in HEART_CATEGORICAL:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].astype(\"category\")\n",
    "    for col in HEART_BINARIES + HEART_NUMERIC:\n",
    "        for c in [col]:\n",
    "            if c in X.columns:\n",
    "                X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "    # Your copy shows no NaNs; if needed later we can add SimpleImputer.\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = _split_stratified(X, y)\n",
    "    preproc = _build_preprocessor_heart(X)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, preproc\n",
    "\n",
    "# --- Manual smoke test: `python -m src.load`\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, pre = load_heart()\n",
    "    def pct_pos(y): \n",
    "        return f\"{(y.mean() if len(y) else float('nan')):.3f}\"\n",
    "    print(\"Splits (rows):\", len(X_train), len(X_val), len(X_test))\n",
    "    print(\"Pos rate (train/val/test):\", pct_pos(y_train), pct_pos(y_val), pct_pos(y_test))\n",
    "    pipe = Pipeline([(\"pre\", pre)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    Xt = pipe.transform(X_train)\n",
    "    print(\"Transformed train shape:\", Xt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab75c12-ebfb-400d-9363-2a58533d1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PIMA (Indians Diabetes) ----------\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PIMA_ZERO_IS_NAN = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "\n",
    "def _build_preprocessor_pima(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    # All columns are numeric; we’ll impute and scale.\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    numeric_cols = X.columns.tolist()\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline(steps=[\n",
    "                (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scale\", StandardScaler())\n",
    "            ]), numeric_cols)\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return preproc\n",
    "\n",
    "def load_pima(csv_path: Path | None = None) -> Tuple[pd.DataFrame, ...]:\n",
    "    \"\"\"\n",
    "    Load and prep the Pima Indians Diabetes dataset (Outcome is the label).\n",
    "    - Replaces biologically-impossible zeros with NaN in selected columns\n",
    "    - Stratified 60/20/20 split\n",
    "    - Returns: X_train, X_val, X_test, y_train, y_val, y_test, preprocessor\n",
    "    \"\"\"\n",
    "    if csv_path is None:\n",
    "        csv_path = DATA_DIR / \"diabetes.csv\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Label\n",
    "    if \"Outcome\" not in df.columns:\n",
    "        raise ValueError(\"Expected label column 'Outcome' in diabetes.csv\")\n",
    "    y = df[\"Outcome\"].astype(int)\n",
    "    X = df.drop(columns=[\"Outcome\"]).copy()\n",
    "\n",
    "    # Replace zeros with NaN in the known problematic fields\n",
    "    for col in PIMA_ZERO_IS_NAN:\n",
    "        if col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "            X.loc[X[col] == 0, col] = np.nan\n",
    "\n",
    "    # Ensure numeric dtype for all features\n",
    "    for c in X.columns:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "    # Stratified 60/20/20 split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, stratify=y, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n",
    "    )\n",
    "\n",
    "    preproc = _build_preprocessor_pima(X)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, preproc\n",
    "\n",
    "# --- Optional quick check: `python -m src.load_pima_check`\n",
    "if __name__ == \"__main__\" and False:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, pre = load_pima()\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipe = Pipeline([(\"pre\", pre)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(\"Pima splits:\", len(X_train), len(X_val), len(X_test))\n",
    "    print(\"Pima pos rates:\", y_train.mean(), y_val.mean(), y_test.mean())\n",
    "    print(\"Transformed shape (train):\", pipe.transform(X_train).shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Mar 12 2025, 20:22:46) \n[Clang 17.0.0 (clang-1700.0.13.3)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
